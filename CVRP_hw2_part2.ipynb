{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W37lUOmT9yle"
   },
   "source": [
    "# CVRP HW2 - part 2\n",
    "In this part, you will implement **unconditional** and **conditional** PixelCNN on MNIST dataset.\n",
    "To have faster training efficency, GPU acceleration is recommended. However, don't worry if you reach your GPU usage limit on Colab. This part can still be completed within a reasonable timeframe using a standard CPU.\n",
    "\n",
    "PixelCNNs are a type of autoregressive generative model that generate images by modeling the joint distribution of pixels as a product of conditional distributions. In PixelCNN, the pixels are generated one by one in a specific order, with each pixel conditioned on the previously generated pixels. The model uses masked convolutions to ensure that the prediction of each pixel only depends on the already generated pixels, following the raster scan order. For more details, please refer to Lecture 13.\n",
    "\n",
    "Some notes:\n",
    "- This part takes 50 points in total\n",
    "- There is **no questions** in this part\n",
    "- If you made any modifications on the cells with \"DO NOT MODIFY\" label, please be sure to write a detailed README, otherwise, TA will have a hard time grading your code...\n",
    "- If you find it's hard to put all the things into a single notebook, you can have additional files. Make sure you have a README as well.\n",
    "- Please zip all the notebooks or files you have to a compressed zip file before submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-iydqiNU5fGu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb2COTm-ocln"
   },
   "source": [
    "### MNIST dataset loader\n",
    "Please do not modify this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-J1xVnVE_6Zm"
   },
   "outputs": [],
   "source": [
    "# NOTE: DO NOT MODIFY\n",
    "\n",
    "def binarize_image(tensor):\n",
    "    return (tensor > 0.5).float()\n",
    "\n",
    "tensor_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(binarize_image)\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "train_dataset = datasets.MNIST(root = \"./data\",\n",
    "\t\t\t\t\t\t\ttrain = True,\n",
    "\t\t\t\t\t\t\tdownload = True,\n",
    "\t\t\t\t\t\t\ttransform = tensor_transform)\n",
    "test_dataset = datasets.MNIST(root = \"./data\",\n",
    "\t\t\t\t\t\t\ttrain = False,\n",
    "\t\t\t\t\t\t\tdownload = True,\n",
    "\t\t\t\t\t\t\ttransform = tensor_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "\t\t\t\t\t\t\t\t\t\tbatch_size = batch_size,\n",
    "\t\t\t\t\t\t\t\t\t\tshuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "\t\t\t\t\t\t\t\t\t\tbatch_size = batch_size,\n",
    "\t\t\t\t\t\t\t\t\t\tshuffle = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_5PFYwtohYY"
   },
   "source": [
    "## **PixelCNN**\n",
    "\n",
    "Tasks:\n",
    "- [15 pts] Implement Masked Convolution by finishing the `MaskedConv2d()` function\n",
    "- [15 pts] Implement PixelCNN with your `MaskedConv2d()`\n",
    "- The training function for unconditional PixelCNN is provided, there is no need to modify `train()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDICmZADIiUE"
   },
   "outputs": [],
   "source": [
    "bce = F.binary_cross_entropy\n",
    "\n",
    "def train(dataloader, model, optimizer, epochs):\n",
    "    losses = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for iter, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            tgt = images.clone()\n",
    "            pred = model(images)\n",
    "            loss = bce(pred, tgt)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            avg_loss = running_loss * batch_size / len(train_dataset)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        tqdm.write(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0mqbPsCopVJ"
   },
   "source": [
    "### Masked Convolution\n",
    "The `MaskedConv2d()` function applies a mask to the convolutional filters in a 2D convolutional layer. The mask ensures that each pixel is only influenced by the pixels that have been generated before it. The function should allow for both type A and type B masks, where type A masks do not allow the central pixel to influence itself, and type B masks do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46O03X-498n0"
   },
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self):\n",
    "        super().__init__(in_channels=1, out_channels=64, kernel_size=7, padding=3)\n",
    "        \n",
    "        # Create mask\n",
    "        mask = torch.ones_like(self.weight)\n",
    "        _, _, height, width = self.weight.shape\n",
    "        \n",
    "        # Calculate center positions\n",
    "        center_h = height // 2\n",
    "        center_w = width // 2\n",
    "        \n",
    "        # Apply mask: mask out future pixels\n",
    "        # For each position, we only allow connections to previous pixels and pixels above\n",
    "        for h in range(height):\n",
    "            for w in range(width):\n",
    "                if h > center_h:  # If we're below the center, mask out\n",
    "                    mask[:, :, h, w] = 0\n",
    "                elif h == center_h:  # If we're on the same row as center\n",
    "                    if w > center_w:  # Mask out future positions\n",
    "                        mask[:, :, h, w] = 0\n",
    "                    elif w == center_w:  # At center position\n",
    "                        mask[:, :, h, w] = 0  # Type A mask (center pixel masked)\n",
    "        \n",
    "        # Register mask as buffer (won't be updated during backprop)\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply mask to weights before convolution\n",
    "        self.weight.data *= self.mask\n",
    "        return super().forward(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYUg7AjHottB"
   },
   "source": [
    "### Implement PixelCNN\n",
    "Using your `MaskedConv2d()` function, implement the PixelCNN architecture. Your implementation should include several masked convolutional layers, nonlinear activations, and any necessary normalization layers. Ensure that your model is capable of generating images pixel by pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrG8n5WJAo51"
   },
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial masked convolution layer\n",
    "        self.masked_conv = MaskedConv2d()\n",
    "        \n",
    "        # Middle layers with residual connections\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(64, 64, kernel_size=7, padding=3),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 64, kernel_size=7, padding=3),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU()\n",
    "            ) for _ in range(7)  # 7 residual blocks\n",
    "        ])\n",
    "        \n",
    "        # Final layers to produce output\n",
    "        self.output_layers = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1),\n",
    "            nn.Sigmoid()  # For binary images\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial masked convolution\n",
    "        out = self.masked_conv(x)\n",
    "        \n",
    "        # Residual blocks\n",
    "        for conv_block in self.conv_layers:\n",
    "            residual = out\n",
    "            out = conv_block(out)\n",
    "            out = out + residual  # Residual connection\n",
    "            out = F.relu(out)\n",
    "        \n",
    "        # Final layers\n",
    "        out = self.output_layers(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2KxDaEBo-9Z"
   },
   "source": [
    "### Training\n",
    "Train your PixelCNN model on the MNIST dataset. We will use Cross-Entropy Loss (BCE) as the training objective. This is equal to maximizing likelihood for binary sequence autoregressive models. After training, evaluate the model using the provided code for unconditional generation of MNIST digits. You may need to adjust hyperparameters such as the learning rate, number of epochs, and batch size to achieve good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_n_2PGRUAzou"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "model = PixelCNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "log = train(train_loader, model, optimizer, epochs)\n",
    "\n",
    "# added code to save the model to pixelcnn_mnist.pth\n",
    "model_save_path = 'pixelcnn_mnist.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epochs': epochs,\n",
    "    'loss': log\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQnZFPm_pDkc"
   },
   "source": [
    "### Evaluation (Reconstruction)\n",
    "The following code is to verify your PixelCNN is able to reconstruct the image from MNIST test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFFyeeEjxsxQ"
   },
   "outputs": [],
   "source": [
    "H, W = 28, 28\n",
    "\n",
    "with torch.no_grad():\n",
    "  for iter, (images, labels) in enumerate(test_loader):\n",
    "      images = images.to(device)\n",
    "      pred = model(images)\n",
    "\n",
    "      for i in range(H):\n",
    "          for j in range(W):\n",
    "              pred[:, :, i, j] = torch.bernoulli(pred[:, :, i, j], out=pred[:, :, i, j])\n",
    "      break\n",
    "\n",
    "samples = pred.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "fig, axes = plt.subplots(8, 8, figsize=(15, 15))\n",
    "\n",
    "for i in range(64):\n",
    "    sample = samples[i]\n",
    "    row, col = divmod(i, 8)\n",
    "    axes[row, col].imshow(sample, cmap='gray')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqr0yIvYpL2_"
   },
   "source": [
    "### Evaluation (Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQseF8t5BpPU"
   },
   "outputs": [],
   "source": [
    "samples = torch.zeros(size=(64, 1, H, W)).to(device)\n",
    "with torch.no_grad():\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            if j > 0 and i > 0:\n",
    "                out = model(samples)\n",
    "                samples[:, :, i, j] = torch.bernoulli(out[:, :, i, j], out=samples[:, :, i, j])\n",
    "\n",
    "samples = samples.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "fig, axes = plt.subplots(8, 8, figsize=(15, 15))\n",
    "\n",
    "for i in range(64):\n",
    "    sample = samples[i]\n",
    "    row, col = divmod(i, 8)\n",
    "    axes[row, col].imshow(sample, cmap='gray')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "252Jr5orpZGc"
   },
   "source": [
    "## **Conditional PixelCNN**\n",
    "Conditional PixelCNNs extend the basic PixelCNN model by conditioning the generation process on additional information, such as class labels. In this part, you will implement and train a Conditional PixelCNN on the MNIST dataset, where the model is conditioned on the digit class labels.\n",
    "\n",
    "Tasks:\n",
    "- [5 pts] Extend your implementation of the `ConditionalMaskedConv2d()` function to incorporate a conditional bias based on class labels\n",
    "- [5 pts] Using your `ConditionalMaskedConv2d()` function, implement the Conditional PixelCNN architecture\n",
    "- [5 pts] Write your own training function. The training process is similar as in unconditional PixelCNN with some changes\n",
    "- [5 pts] Train your Conditional PixelCNN on the MNIST dataset. After training, evaluate the model’s performance on the test set by generating images conditioned on specific digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKgicaMje-hL"
   },
   "outputs": [],
   "source": [
    "def train_cond(dataloader, model, optimizer, epochs, n_classes):\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for iter, (images, labels) in enumerate(dataloader):\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Convert labels to one-hot encoding\n",
    "            labels_onehot = F.one_hot(labels, num_classes=n_classes).float()\n",
    "            \n",
    "            # Get target images\n",
    "            tgt = images.clone()\n",
    "            \n",
    "            # Forward pass with conditional input\n",
    "            pred = model(images, labels_onehot)\n",
    "            \n",
    "            # Calculate binary cross entropy loss\n",
    "            loss = bce(pred, tgt)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate average loss\n",
    "            avg_loss = running_loss * batch_size / len(train_dataset)\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        tqdm.write(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nh9gGbKUpfFy"
   },
   "source": [
    "### Conditional Masked Convolution\n",
    "The conditional bias should be added to the output of each convolutional layer $l$ as follows:\n",
    "\n",
    "$W_l * x + b_l + V_{ly}$\n",
    "\n",
    "where $W_l * x + b_l$ represents the masked convolution, $V_l$ is a 2D weight matrix, and $y$ is the one-hot encoded class label. The conditional bias $V_{ly}$ should be broadcasted spatially and added to the output channel-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUEQ-RzyHdrs"
   },
   "outputs": [],
   "source": [
    "class ConditionalMaskedConv2d(MaskedConv2d):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # V_l matrix: maps from n_classes (10) to out_channels (64)\n",
    "        self.conditional_weights = nn.Parameter(torch.randn(64, 10))  # V_l matrix\n",
    "        \n",
    "        # Initialize the conditional weights using Xavier/Glorot initialization\n",
    "        nn.init.xavier_uniform_(self.conditional_weights)\n",
    "\n",
    "    def forward(self, x, class_condition):\n",
    "        # Apply the masked convolution from parent class\n",
    "        # W_l * x + b_l\n",
    "        conv_out = super().forward(x)\n",
    "        \n",
    "        # Calculate V_l * y\n",
    "        # class_condition shape: [batch_size, n_classes]\n",
    "        # conditional_weights shape: [out_channels, n_classes]\n",
    "        # conditional_bias shape: [batch_size, out_channels]\n",
    "        conditional_bias = torch.matmul(class_condition, self.conditional_weights.t())\n",
    "        \n",
    "        # Reshape conditional bias to match convolution output\n",
    "        # [batch_size, out_channels, 1, 1]\n",
    "        conditional_bias = conditional_bias.unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        # Add conditional bias to convolution output\n",
    "        # Broadcasting will handle spatial dimensions\n",
    "        return conv_out + conditional_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYhdWZdZpjTy"
   },
   "source": [
    "### Implement Conditional PixelCNN\n",
    "Using your `ConditionalMaskedConv2d()` function, implement the Conditional PixelCNN architecture. Your model should include several conditional masked convolutional layers, with the class label influencing each layer’s output through the conditional bias. The overall architecture should closely resemble the regular PixelCNN but with the additional conditioning on class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjICYyhWeZUI"
   },
   "outputs": [],
   "source": [
    "class ConditionalPixelCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial conditional masked convolution layer\n",
    "        self.masked_conv = ConditionalMaskedConv2d()\n",
    "        \n",
    "        # Middle layers with residual connections\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(64, 64, kernel_size=7, padding=3),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 64, kernel_size=7, padding=3),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU()\n",
    "            ) for _ in range(7)  # 7 residual blocks\n",
    "        ])\n",
    "        \n",
    "        # Final layers to produce output\n",
    "        self.output_layers = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1),\n",
    "            nn.Sigmoid()  # For binary images\n",
    "        )\n",
    "\n",
    "    def forward(self, x, class_condition):\n",
    "        # Initial masked convolution with conditioning\n",
    "        out = self.masked_conv(x, class_condition)\n",
    "        \n",
    "        # Residual blocks\n",
    "        for conv_block in self.conv_layers:\n",
    "            residual = out\n",
    "            out = conv_block(out)\n",
    "            out = out + residual  # Residual connection\n",
    "            out = F.relu(out)\n",
    "        \n",
    "        # Final layers\n",
    "        out = self.output_layers(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0V3Skf2mpoN5"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXkK2VKIeo3K"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "n_classes = 10\n",
    "model = ConditionalPixelCNN(n_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "log = train_cond(train_loader, model, optimizer, epochs, n_classes)\n",
    "\n",
    "# added code to save the model to conditional_pixelcnn_mnist.pth\n",
    "model_save_path = 'conditional_pixelcnn_mnist.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epochs': epochs,\n",
    "    'n_classes': n_classes,\n",
    "    'loss': log\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epotdWhTpo62"
   },
   "source": [
    "### Evaluation (Reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLW9jRCyfiOd"
   },
   "outputs": [],
   "source": [
    "H, W = 28, 28\n",
    "\n",
    "with torch.no_grad():\n",
    "  for iter, (images, labels) in enumerate(test_loader):\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      labels = F.one_hot(labels, num_classes=n_classes).float()\n",
    "      pred = model(images, labels)\n",
    "\n",
    "      for i in range(H):\n",
    "          for j in range(W):\n",
    "              pred[:, :, i, j] = torch.bernoulli(pred[:, :, i, j], out=pred[:, :, i, j])\n",
    "      break\n",
    "\n",
    "samples = pred.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "fig, axes = plt.subplots(8, 8, figsize=(15, 15))\n",
    "\n",
    "for i in range(64):\n",
    "    sample = samples[i]\n",
    "    row, col = divmod(i, 8)\n",
    "    axes[row, col].imshow(sample, cmap='gray')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvOHwS6yptWn"
   },
   "source": [
    "### Evaluation (Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIGipU7ofRJF"
   },
   "outputs": [],
   "source": [
    "samples = torch.zeros(size=(60, 1, H, W)).to(device)\n",
    "sample_classes = np.sort(np.array([np.arange(n_classes)] * 6).flatten())\n",
    "sample_classes = F.one_hot(torch.tensor(sample_classes), num_classes=n_classes).to(device).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            if j > 0 and i > 0:\n",
    "                out = model(samples, sample_classes)\n",
    "                samples[:, :, i, j] = torch.bernoulli(out[:, :, i, j], out=samples[:, :, i, j])\n",
    "\n",
    "samples = samples.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "fig, axes = plt.subplots(10, 6, figsize=(15, 30))\n",
    "\n",
    "for i in range(60):\n",
    "    sample = samples[i]\n",
    "    row, col = divmod(i, 6)\n",
    "    axes[row, col].imshow(sample, cmap='gray')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
